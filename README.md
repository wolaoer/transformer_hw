# transformer_hw
Hereâ€™s a concise and clear **README.md** for your repository:

---

# Transformer Model Research Report

This project implements a simplified **Transformer Decoder** model using **PyTorch**, based on the architecture proposed in *â€œAttention Is All You Needâ€* (Vaswani et al., 2017). The implementation includes training, evaluation, ablation studies, and text generation experiments on the **WikiText-2** dataset.

---

## ğŸ“˜ Overview

* Implements **multi-head self-attention**, **positional encoding**, and **feed-forward networks** from scratch.
* Conducts **ablation experiments** (no attention / no positional encoding).
* Supports **text generation** and **perplexity evaluation**.
* Uses **ModelScope** for dataset loading and **Qwen tokenizer** for text processing.

---

## ğŸ§© Repository Structure

```
project_root/
â”‚â”€â”€ dataset/                # Raw and preprocessed datasets
â”‚â”€â”€ src/                 # Training and evaluation scripts
â”‚   â”‚â”€â”€ model.py         # Transformer model implementation
â”‚   â”‚â”€â”€ train.py         # Training loop
â”‚   â”‚â”€â”€ generate.py          # Evaluation and generation
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/wolaoer/transformer_hw.git
cd transformer_hw
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### Train the model

```bash
python transformer_hw.py
```

### Generate text samples

```bash
python transformer_hw.py --generate
```

---

## ğŸ§  Experiments

* **Dataset:** WikiText-2 (via ModelScope)
* **Optimizer:** Adam, LR = 1e-3
* **Batch size:** 16
* **Epochs:** 10
* **Model:** 2 layers, 4 heads, embedding dim = 128, FFN dim = 512

---

## ğŸ“Š Results (Example)

| Model Variant          | Val Loss | Perplexity |
| ---------------------- | -------- | ---------- |
| Baseline (Full)        | 5.21     | 184.08     |
| No Positional Encoding | â†‘        | â†‘          |
| No Attention           | â†‘â†‘       | â†‘â†‘         |

---

## ğŸ§° Environment

* **Python:** 3.10
* **PyTorch:** 2.6.0 + CUDA 12.4
* **GPU:** NVIDIA A100 (80GB)

---

## ğŸ“ Reference

Vaswani et al., *â€œAttention Is All You Needâ€*, NeurIPS 2017.
[GitHub Repository](https://github.com/wolaoer/transformer_hw)

---
